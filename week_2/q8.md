###Exercise 8. Block aggregation in PageRank (exercise 3.4 in book)

#####Set θ = 0.85 and start with any normalized initial vector π[0].
#####a) Compute the PageRank vector $[π_A^∗ π_B^∗ ]^T$ of the graph in figure (a) above with $$H = \begin{pmatrix}1&0\\1/3&2/3\end{pmatrix}$$

#####Note the uneven splitting of link weights from node B. This will be useful later in the problem.

Since there is no dangling nodes, $\hat H = H$

$$G = \theta \hat H + (1-\theta) \frac{1}{N}11^T\\
=0.85\begin{pmatrix}1&0\\1/3&2/3\end{pmatrix} +0.15 \begin{pmatrix}1/2&1/2\\1/2&1/2\end{pmatrix}\\
=\begin{pmatrix}0.925&0.075\\0.35833&0.64166\end{pmatrix}$$

Let $\pi[0]^T = [1/2\ \ 1/2]^T$

Since $\pi[i]^T = \pi[i-1]^T*G$, after iterating through the formula starting from i = 0, the pagerank vector converges to a value of $$\pi^*_{AB} = [\pi^*_A\ \pi^*_B]^T = \begin{pmatrix}0.82671998&0.17328002\end{pmatrix}^T$$


#####b) Compute the PageRank vectors $[π_1^∗ π_2^∗]^T $and $[π_3^∗ π_4^∗ π_5^∗]^T$ of the two graphs in figure (b) above.

Graph 1

$$H_{12} = \begin{pmatrix}0&1\\1&0\end{pmatrix}$$

Since there is no dangling nodes, $\hat H_{12} = H_{12}$

$$G_{12} = \begin{pmatrix}0.925&0.075\\0.075&0.925\end{pmatrix}$$

$$\pi^*_{12} = [\pi^*_1\ \pi^*_2]^T =\begin{pmatrix}0.5&  0.5\end{pmatrix}^T$$

Graph 2

$$H_{345} = \begin{pmatrix}1/2&0&1/2\\1/2&0&1/2\\0&0&0\end{pmatrix}$$

$$\hat H_{345} = \begin{pmatrix}1/2&0&1/2\\1/2&0&1/2\\1/3&1/3&1/3\end{pmatrix}$$

$$G_{345} = \begin{pmatrix}0.475&0.05&0.475\\0.475&0.05&0.475\\1/3&1/3&1/3\end{pmatrix}$$

$$\pi^*_{345} = [\pi^*_3\ \pi^*_4\ \pi^*_5]^T =\begin{pmatrix}0.41439815&0.1712037&0.41439815\end{pmatrix}^T$$

#####c) If we divide the graph in exercise 7 into two blocks as shown in the figure below, we can approximate $π^∗$ in the previous question by
$$\hat π^∗ = [π_A^∗ · [π_1^∗ π_2^∗] π_B^∗ · [π_3^∗ π_4^∗ π_5^∗]]^T $$.
#####Compute this vector. Explain the advantage, in terms of computational load, of using this approximation instead of directly computing $π^∗$.

$$\hat π^∗ = [0.82671998 · [0.5\ \ 0.5]\ \ \ 0.17328002 · [0.41439815\ \ 0.1712037\ \ 0.41439815]]^T \\ 
=[0.41335999\ \ 0.41335999\ \ 0.07180691972\ \ 0.02966618056\ \ 0.07180691972]^T$$

The approximation is considerably less computationally expensive as our matrix operations involves matrices of a smaller size. As matrix multiplication of a m * n matrix and a n * p matrix is $O(mnp)$, or in this case $O(n^2)$ since $m = 1$. As the size of matrix increases, the computational power required would increase quadratically.